<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>MODEL-ENSEMBLE TRUST-REGION POLICY OPTIMIZATION | BLOG</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://perfectqing.github.io/blogs/favicon.ico?v=1626771746855">
<link rel="stylesheet" href="https://perfectqing.github.io/blogs/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="MODEL-ENSEMBLE TRUST-REGION POLICY OPTIMIZATION
摘要：
借助深度学习的最新进展，无模型强化学习方法在越来越多的任务中取得了成功。然而，它们往往遭受高样本复杂性的困扰，这阻碍了它们在现实世界领域..." />
    <meta name="keywords" content="meta-learning" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://perfectqing.github.io/blogs">
        <img src="https://perfectqing.github.io/blogs/images/avatar.png?v=1626771746855" class="site-logo">
        <h1 class="site-title">BLOG</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      Carry on what you want...
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> Copyright © AlsoRan | <a class="rss" href="https://perfectqing.github.io/blogs/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">MODEL-ENSEMBLE TRUST-REGION POLICY OPTIMIZATION</h2>
            <div class="post-date">2021-06-11</div>
            
            <div class="post-content" v-pre>
              <h1 id="model-ensemble-trust-region-policy-optimization">MODEL-ENSEMBLE TRUST-REGION POLICY OPTIMIZATION</h1>
<h2 id="摘要">摘要：</h2>
<p>借助深度学习的最新进展，无模型强化学习方法在越来越多的任务中取得了成功。然而，它们往往遭受高样本复杂性的困扰，这阻碍了它们在现实世界领域中的应用。或者，基于模型的强化学习<strong>有望降低样本复杂性</strong>，但往往需要仔细调整，迄今为止，它主要在<strong>简单模型足以学习</strong>的限制性领域取得了成功。</p>
<p>在本文中，我们分析了当使用深度神经网络来学习模型和策略时，基于普通模型的强化学习方法的行为，并且我们表明，学习的策略倾向于利用数据不足的区域来学习模型，导致训练不稳定。为了克服这个问题，我们建议使用一组模型来保持模型的不确定性，并规范学习过程。我们进一步表明，似然比导数的使用比时间的反向传播产生更稳定的学习。</p>
<p>总之，我们的方法模型集成信任区域策略优化(ME-TRPO)与无模型深度RL方法相比，在挑战连续控制基准任务<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow></mrow><mrow><mn>1</mn><mo separator="true">,</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{1,2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>上<strong>显著降低了样本复杂性</strong>。</p>
<h2 id="1-introduction">1 Introduction</h2>
<p>深度强化学习近年来取得了很多令人印象深刻的成果，这些结果中的许多是使用无模型强化学习算法实现的，该算法不试图构建环境模型。相比之下，基于模型的强化学习算法利用环境的已学习到的模型来辅助学习。这些方法可能比无模型算法具有更高的样本效率，因此可以应用于低样本复杂度至关重要的现实任务。然而，到目前为止，这样的方法需要<strong>一个关于学习模型的非常严格的形式</strong>，以及为使它们适用而进行的仔细调整。虽然将基于模型的算法扩展到深度神经网络模型是一个简单的想法，但迄今为止成功的应用相对较少。</p>
<p>基于模型的强化学习的标准方法在模型学习和策略优化之间交替。在模型学习阶段，从与环境的交互中收集样本，并使用监督学习来将动力学模型拟合到观察值。在策略优化阶段，学习的模型用于搜索改进的策略。</p>
<p>虽然普通的基于模型的反向学习可以在低维任务中以相对简单的动力学很好地工作，但我们发现在更具挑战性的连续控制任务中，性能非常不稳定。当使用诸如深度神经网络的表达模型时，这个问题严重恶化。</p>
<p>为了解决这个问题，我们建议使用深度神经网络的集合来保持模型的不确定性，给定从环境中收集的数据。在模型学习过程中，我们通过改变神经网络的权值初始化和训练输入序列来区分神经网络。然后，在策略学习期间，我们通过结合来自想象的随机展开的梯度来调整策略更新。每个想象的步骤都是从系综预测中统一采样的。使用这种技术，策略学会了在实际环境中遇到的各种可能的情况下变得健壮。为了避免过度适应这个正则化目标，我们使用模型集成进行早期停止策略训练。</p>
<p>标准的基于模型的技术需要在许多时间步骤上对模型进行区分，这一过程被称为时间反向传播(BPTT)。众所周知，在文献中，BPTT可以导致爆炸和消失梯度(Hochreiter，1991；Bengio等人，1994年)。即使应用了梯度裁剪，BPTT仍然会陷入糟糕的局部最优。我们建议使用似然比方法代替BPTT来估计梯度，这仅利用模型作为模拟器，而不是用于直接梯度计算。特别是，我们使用信任区策略优化(TRPO)(舒尔曼等人，2015)，该策略对策略施加信任区约束，以进一步稳定学习。</p>
<p>在这项工作中，我们提出了模型集成信任区域策略优化(ME-TRPO)，这是一种基于模型的算法，其性能与最先进的无模型算法相同，样本复杂度降低了100倍。我们证明了模型集成技术是克服基于模型的强化学习中模型偏差挑战的有效方法。我们证明用TRPO代替BPTT可以产生更稳定的学习和更好的最终性能。最后，我们用神经网络作为函数逼近器，对基于典型模型的线性回归进行了实证分析，并指出其在应用于具有挑战性的连续控制任务时的缺陷。</p>
<h2 id="2-related-work">2 Related Work</h2>
<p>已经有大量基于模型的强化学习的工作。它们的不同之处在于模型参数化的选择，这与利用模型进行政策学习的不同方式有关。有趣的是，迄今为止最令人印象深刻的机器人学习应用是使用最简单的模型参数化实现的，即线性模型(巴内尔&amp;施耐德，2001；Abbeel等人，2006年；Levine  &amp;  Abbeel，2014；Watter等人，2015年；Levine等人，2016年a；库马尔等人，2016)，其中模型或者直接在原始状态上运行，或者在状态的特征表示上运行。这种模型是非常数据有效的，并且允许通过来自最优控制的技术进行非常有效的策略优化。然而，它们只有有限的表达能力，不能很好地扩展到复杂的非线性动力学或高维状态空间，除非使用单独的特征学习阶段(Watter等人，2015)。</p>
<p>另一种方法是使用非参数模型，如高斯过程(GPs)(拉斯姆森等人，2003年；Ko等人，2007年；Deisenroth &amp;  Rasmussen，2011)。这种模型可以有效地保持预测的不确定性，并且只要有足够的数据可用，就具有无限的表示能力。然而，它们遭受着维度的诅咒，到目前为止，它们的应用仅限于相对低维的环境。将GPs的不确定性估计纳入政策更新的计算费用也带来了额外的挑战。</p>
<p>深度神经网络在将无模型强化学习算法升级到具有挑战性的场景方面取得了巨大成功(Mnih等人，2015；Silver等人，2016年；舒尔曼等人，2015年；2016).然而，在将它们应用于基于模型的逆向物流方面取得的成功有限。尽管许多先前的研究已经在相对简单的领域显示出有希望的结果(Nguyen  &amp;Wi卓尔，1990；Schmidhuber &amp;  Huber，1991；约旦&amp;鲁梅尔哈特，1992年；Gal等人，2016)，到目前为止，他们在更具挑战性领域的应用要么需要与无模型技术相结合(Oh等人，2015；Heess等人，2015年；Nagabandi等人，2017年)，或特定领域的政策学习或规划算法(Lenz等人，2015年；Agrawal等人，2016年；Pinto  &amp; Gupta，2016；Levine等人，2016bFinn &amp;  Levine，2017；奈尔等人，2017年)。在这项工作中，我们表明，与结合基于模型和无模型元素的方法相比，我们的纯基于模型的方法提高了样本的复杂性。</p>
<p>最近的两项研究显示了一种更普遍适用的基于模型的反向链路算法的良好迹象。Depeweg等人(2017)利用贝叶斯神经网络(BNNs)学习动态模型上的分布，并使用基于梯度的优化在从该分布采样的模型集合上训练策略。Mishra等人(2017)在轨迹的时间延伸段上学习潜在变量动态模型，并在潜在空间上使用基于梯度的优化来训练策略。这两种方法都可以在算法开始运行之前收集的固定样本数据集上工作。因此，他们的评估仅限于随机探索足以收集模型学习数据的领域。相比之下，我们的方法利用了交替执行模型学习和策略学习的迭代过程，因此可以应用于更具挑战性的领域。此外，我们提出的改进与这两种方法都是正交的，并且有可能结合起来产生更好的结果。</p>
<h2 id="3-preliminaries">3 Preliminaries</h2>
<p>强化学习相关</p>
<h2 id="4-vanilla-model-based-deep-reinforcement-learning">4 Vanilla model-based deep reinforcement learning</h2>
<p>在最成功的无模型强化学习方法中，我们迭代地收集数据，估计策略的梯度，改进策略，然后丢弃数据。相反，基于模型的强化学习更广泛地使用数据；它使用收集的所有数据来训练环境动态的模型。训练好的模型可以作为训练策略的模拟器，也可以提供梯度信息(萨顿，1990；Deisenroth  &amp;  Rasmussen，2011；Depeweg等人，2017年；萨顿，1991)。在下一节中，我们描述了基于普通模型的强化学习算法(见算法1)。我们假设模型和策略由神经网络表示，但是该方法对于其他类型的函数逼近器是有效的。</p>
<h3 id="41-model-learning">4.1 Model learning</h3>
<p>过渡动态用前馈神经网络建模，使用标准实践来训练神经网络以预测给定状态和动作作为输入的状态(而不是下一个状态)的变化。这缓解了神经网络对输入状态的记忆，尤其是在变化很小时(Deisenroth  &amp; Rasmussen，2011；傅等，2016；Nagabandi等人，2017年)。我们表示下一个状态的函数逼近器，它是神经网络的输入状态和输出的和。模型学习的目标是找到使L21步预测损失最小的参数φ3:其中D是存储代理经历的转换的训练数据集。我们使用亚当优化器(Kingma &amp;  Ba，2014)来解决这个监督学习问题。遵循标准技术以避免过度拟合并促进学习，例如分离验证数据集以提前停止训练，以及将神经网络的输入和输出标准化4</p>
<h3 id="42-policy-learning">4.2 Policy learning</h3>
<p>策略学习</p>
<h2 id="5-model-ensemble-trust-region-policy-optimization">5 Model-Ensemble Trust-Region policy optimization</h2>
<p>使用第4节中描述的普通方法，我们发现学习的策略通常利用动力学模型可用的稀缺训练数据的区域。因为我们在改进关于近似MDP而不是真实的政策，那么对政策有利的预测可能是错误的。这种过拟合问题可以通过以类似于监督学习的方式提前停止使用验证初始状态来部分缓解。然而，我们发现这是不够的，因为性能仍然是使用相同的学习模型来评估的，这往往会犯一致的错误。此外，虽然梯度剪辑通常可以解决爆炸梯度，BPTT仍然遭受消失梯度，这导致政策陷入糟糕的局部最优(本吉奥等人，1994；Pascanu等人，2013年)。这些问题在长视野优化时尤为严重，这在强化学习问题中非常常见。</p>
<p>我们现在提出我们的方法，模型集成信任区域策略优化。伪代码如算法2所示。ME-TRPO结合了对普通方法的三种修改。首先，我们拟合一组动力学模型{fφ1，.。。使用相同的真实世界数据。这些模型通过标准监督学习进行训练，如第4.1节所述，它们的区别仅在于初始权重和小批量样本的采样顺序。其次，我们使用信任区域策略优化(TRPO)来优化模型集成上的策略。第三，我们使用模型集成来监控策略在验证数据上的性能，并在策略停止改进时停止当前迭代。下面详细描述第二和第三修改。</p>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://perfectqing.github.io/blogs/tag/jgVN7rH4D/" class="tag">
                    meta-learning
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://perfectqing.github.io/blogs/post/hello-gridea/">
                  <h3 class="post-title">
                    Hello Gridea
                  </h3>
                </a>
              </div>
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>


  <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad()
  </script>





  </body>
</html>
